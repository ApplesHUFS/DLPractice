{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 인공신경망 학습 코드 (Neural Network Training Code)\n",
    "\n",
    "### 기본 설정 (Basic Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현성을 위한 시드 설정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스 (Using device): {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토이 데이터셋 생성 (Create Toy Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2클래스 분류 문제를 위한 토이 데이터 생성\n",
    "def generate_toy_data(n_samples=1000):\n",
    "    # 두 개의 원형 클러스터 생성\n",
    "    n_samples_per_class = n_samples // 2\n",
    "    \n",
    "    # 클래스 0 데이터 \n",
    "    X0 = np.random.randn(n_samples_per_class, 2) * 0.5 + np.array([2, 2])\n",
    "    y0 = np.zeros(n_samples_per_class)\n",
    "    \n",
    "    # 클래스 1 데이터\n",
    "    X1 = np.random.randn(n_samples_per_class, 2) * 0.5 + np.array([-2, -2])\n",
    "    y1 = np.ones(n_samples_per_class)\n",
    "    \n",
    "    # 데이터 합치기\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([y0, y1])\n",
    "    \n",
    "    # 데이터 섞기\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X, y = X[indices], y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 데이터 생성\n",
    "X, y = generate_toy_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', label='클래스 0')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='red', label='클래스 1')\n",
    "plt.title('분류 문제를 위한 토이 데이터셋 ')\n",
    "plt.xlabel('특성 1 (Feature 1)')\n",
    "plt.ylabel('특성 2 (Feature 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 학습/테스트로 분할\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(X) * train_ratio)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(f\"학습 데이터 크기 (Training data size): {X_train_tensor.shape}\")\n",
    "print(f\"테스트 데이터 크기 (Test data size): {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 모델 정의 (Define Neural Network Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 피드포워드 신경망 (Simple feedforward neural network)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        모델 구조 초기화 (Initialize model architecture)\n",
    "        \"\"\"\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # 레이어 정의 (Define layers)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # 이진 분류를 위한 시그모이드 (Sigmoid for binary classification)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파(Forward pass, 역전파는 자동계산, backward() 호출로 수행)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성 (Create model instance)\n",
    "input_size = 2        # 입력 특성 수 (Number of input features)\n",
    "hidden_size = 16      # 은닉층 크기 (Hidden layer size)\n",
    "output_size = 1       # 출력 크기 (이진 분류) (Output size - binary classification)\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 옵티마이저 정의 (Define loss function and optimizer)\n",
    "criterion = nn.BCELoss()  # 이진 교차 엔트로피 손실 (Binary Cross Entropy Loss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 루프 (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정 시각화를 위한 손실 기록 (Record losses for visualization)\n",
    "losses = []\n",
    "\n",
    "# 에포크 수 정의 (Define number of epochs)\n",
    "num_epochs = 100\n",
    "\n",
    "# 학습 루프 (Training loop)\n",
    "for epoch in range(num_epochs):\n",
    "    # 모델을 학습 모드로 설정 (Set model to training mode)\n",
    "    model.train()\n",
    "    \n",
    "    # 순전파 (Forward pass)\n",
    "    outputs = model(X_train_tensor)\n",
    "    outputs = outputs.squeeze()  # 차원 축소 (Reduce dimensions)\n",
    "    \n",
    "    # 손실 계산 (Calculate loss)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # 역전파 및 최적화 (Backpropagation and optimization)\n",
    "    optimizer.zero_grad()  # 기울기 초기화 (Zero gradients)\n",
    "    loss.backward()        # 역전파 (Backpropagation)\n",
    "    optimizer.step()       # 가중치 업데이트 (Update weights)\n",
    "    \n",
    "    # 손실 기록 (Record loss)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 학습 과정 출력 (Print training progress)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        # 테스트 데이터에 대한 성능 평가 (Evaluate on test data)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor).squeeze()\n",
    "            test_loss = criterion(test_outputs, y_test_tensor)\n",
    "            \n",
    "            # 예측값 계산 (Calculate predictions)\n",
    "            train_preds = (outputs > 0.5).float()\n",
    "            test_preds = (test_outputs > 0.5).float()\n",
    "            \n",
    "            # 정확도 계산 (Calculate accuracy)\n",
    "            train_acc = (train_preds == y_train_tensor).float().mean()\n",
    "            test_acc = (test_preds == y_test_tensor).float().mean()\n",
    "            \n",
    "        print(f'에포크 (Epoch) [{epoch+1}/{num_epochs}], '\n",
    "              f'손실 (Loss): {loss.item():.4f}, '\n",
    "              f'학습 정확도 (Train Acc): {train_acc:.4f}, '\n",
    "              f'테스트 정확도 (Test Acc): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과 시각화 (Visualize Training Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 곡선 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('학습 중 손실 변화')\n",
    "plt.xlabel('에포크 (Epoch)')\n",
    "plt.ylabel('손실 (Loss)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 경계 시각화 (Visualize decision boundary)\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # 그리드 생성 (Create a grid)\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # 그리드 포인트에 대한 예측 (Predictions for grid points)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        grid = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]).to(device)\n",
    "        Z = model(grid).squeeze().cpu().numpy()\n",
    "        Z = (Z > 0.5).astype(int)\n",
    "    \n",
    "    # 결정 경계 표시 (Display decision boundary)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # 데이터 포인트 표시 (Display data points)\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='클래스 0')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='클래스 1')\n",
    "    \n",
    "    plt.title('신경망 분류기의 결정 경계 (Decision Boundary of Neural Network Classifier)')\n",
    "    plt.xlabel('특성 1 (Feature 1)')\n",
    "    plt.ylabel('특성 2 (Feature 2)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 (Evaluate model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor).squeeze()\n",
    "    test_preds = (test_outputs > 0.5).float()\n",
    "    test_acc = (test_preds == y_test_tensor).float().mean()\n",
    "    \n",
    "print(f'최종 테스트 정확도 (Final Test Accuracy): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 인코딩 (토큰화, 임베딩) (Text Encoding - Tokenization, Embedding)\n",
    "\n",
    "### 토이 텍스트 데이터 생성 (Create Toy Text Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 텍스트 분류 데이터\n",
    "texts = [\n",
    "    \"이 영화는 정말 재미있었어요\",\n",
    "    \"저는 이 책을 매우 좋아했습니다\",\n",
    "    \"음식이 맛있지 않았어요\",  \n",
    "    \"서비스가 나빴습니다\",  \n",
    "    \"이 제품은 품질이 좋습니다\",\n",
    "    \"가격이 너무 비쌉니다\",\n",
    "    \"이 장소는 아름다웠어요\",\n",
    "    \"날씨가 좋지 않았어요\",\n",
    "    \"직원들이 친절했습니다\",\n",
    "    \"대기 시간이 너무 길었어요\",\n",
    "]\n",
    "\n",
    "# 라벨 생성 (긍정: 1, 부정: 0)\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# 데이터 출력\n",
    "for text, label in zip(texts, labels):\n",
    "    sentiment = \"긍정(Positive)\" if label == 1 else \"부정(Negative)\"\n",
    "    print(f\"텍스트 (Text): {text} | 감정 (Sentiment): {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    간단한 공백 기반 토큰화 (Simple space-based tokenization)\n",
    "    \"\"\"\n",
    "    # 소문자로 변환 (Convert to lowercase)\n",
    "    text = text.lower()\n",
    "    # 기본 전처리 (Basic preprocessing)\n",
    "    for char in '.,!?':\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    # 토큰 분리 (Split tokens)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "\n",
    "# 토큰화 결과 출력 (Print tokenization results)\n",
    "for i, tokens in enumerate(tokenized_texts[:3]):  # 처음 3개만 출력 (Print first 3 only)\n",
    "    print(f\"원본 (Original): {texts[i]}\")\n",
    "    print(f\"토큰화 (Tokenized): {tokens}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전 구축 (Build vocabulary)\n",
    "def build_vocab(tokenized_texts):\n",
    "    \"\"\"\n",
    "    토큰화된 텍스트에서 어휘 사전 생성 (Create vocabulary from tokenized texts)\n",
    "    \"\"\"\n",
    "    # 모든 고유 토큰 수집 (Collect all unique tokens)\n",
    "    vocab = set()\n",
    "    for tokens in tokenized_texts:\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    # 사전 생성 (Create dictionary)\n",
    "    word_to_idx = {word: idx+1 for idx, word in enumerate(sorted(vocab))}\n",
    "    # 패딩을 위한 0 추가 (Add 0 for padding)\n",
    "    word_to_idx['<PAD>'] = 0\n",
    "    # 알 수 없는 토큰을 위한 인덱스 추가 (Add index for unknown tokens)\n",
    "    word_to_idx['<UNK>'] = len(word_to_idx)\n",
    "    \n",
    "    # 역 매핑 사전 생성 (Create reverse mapping)\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "# 어휘 사전 생성 (Create vocabulary)\n",
    "word_to_idx, idx_to_word = build_vocab(tokenized_texts)\n",
    "\n",
    "print(f\"어휘 크기 (Vocabulary size): {len(word_to_idx)}\")\n",
    "print(f\"어휘 사전 샘플 (Vocabulary sample): {dict(list(word_to_idx.items())[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 인덱스로 변환 (Convert text to indices)\n",
    "def text_to_indices(tokenized_text, word_to_idx, max_len=10):\n",
    "    \"\"\"\n",
    "    토큰화된 텍스트를 인덱스 리스트로 변환 (Convert tokenized text to list of indices)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    \n",
    "    # 각 토큰을 인덱스로 변환 (Convert each token to index)\n",
    "    for token in tokenized_text[:max_len]:\n",
    "        if token in word_to_idx:\n",
    "            indices.append(word_to_idx[token])\n",
    "        else:\n",
    "            indices.append(word_to_idx['<UNK>'])\n",
    "    \n",
    "    # 패딩 추가 (Add padding)\n",
    "    while len(indices) < max_len:\n",
    "        indices.append(word_to_idx['<PAD>'])\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# 모든 텍스트를 인덱스로 변환 (Convert all texts to indices)\n",
    "max_len = 10  # 최대 시퀀스 길이 (Maximum sequence length)\n",
    "indices_list = [text_to_indices(tokens, word_to_idx, max_len) for tokens in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력 (Print results)\n",
    "for i in range(3):  # 처음 3개만 출력 (Print first 3 only)\n",
    "    print(f\"원본 (Original): {texts[i]}\")\n",
    "    print(f\"토큰화 (Tokenized): {tokenized_texts[i]}\")\n",
    "    print(f\"인덱스 (Indices): {indices_list[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 인덱스를 PyTorch 텐서로 변환 (Convert indices to PyTorch tensor)\n",
    "X_tensor = torch.LongTensor(indices_list).to(device)\n",
    "y_tensor = torch.FloatTensor(labels).to(device)\n",
    "\n",
    "print(f\"X 텐서 모양 (X tensor shape): {X_tensor.shape}\")\n",
    "print(f\"y 텐서 모양 (y tensor shape): {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 모델 (Embedding Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    임베딩 레이어를 사용한 텍스트 분류 모델 (Text classification model using embedding layer)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        모델 초기화 (Initialize model)\n",
    "        \"\"\"\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        # 임베딩 레이어 (Embedding layer)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM 레이어 (LSTM layer)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                          hidden_dim, \n",
    "                          num_layers=1, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "        # 드롭아웃 (Dropout)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # 출력 레이어 (Output layer)\n",
    "        # 양방향 LSTM이므로 hidden_dim * 2 (Bidirectional LSTM, so hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # 활성화 함수 (Activation function)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        순전파 (Forward pass)\n",
    "        \"\"\"\n",
    "        # 텍스트를 임베딩으로 변환 (Convert text to embeddings)\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # LSTM 레이어에 통과 (Pass through LSTM layer)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        # hidden: [2, batch_size, hidden_dim]\n",
    "        \n",
    "        # 양방향 LSTM의 마지막 은닉 상태 연결 (Concatenate last hidden states of bidirectional LSTM)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # 드롭아웃 적용 (Apply dropout)\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # 출력층에 통과 (Pass through output layer)\n",
    "        output = self.fc(hidden)\n",
    "        # output: [batch_size, output_dim]\n",
    "        \n",
    "        # 시그모이드 활성화 (Sigmoid activation)\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "# 모델 인스턴스 생성 (Create model instance)\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의 (Define loss function and optimizer)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 모델 학습 (Train Text Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/테스트 데이터 분할 (Split training/test data)\n",
    "def train_test_split(X, y, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    데이터를 학습/테스트로 분할 (Split data into train/test)\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    train_size = int(n_samples * train_ratio)\n",
    "    \n",
    "    # 데이터 인덱스 섞기 (Shuffle data indices)\n",
    "    indices = torch.randperm(n_samples)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# 데이터 분할 (Split data)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X_tensor, y_tensor)\n",
    "\n",
    "print(f\"학습 데이터 크기 (Training data size): {X_train.shape}\")\n",
    "print(f\"테스트 데이터 크기 (Test data size): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 (Training loop)\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 모델을 학습 모드로 설정 (Set model to training mode)\n",
    "    model.train()\n",
    "    \n",
    "    # 순전파 (Forward pass)\n",
    "    outputs = model(X_train).squeeze()\n",
    "    \n",
    "    # 손실 계산 (Calculate loss)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # 역전파 및 최적화 (Backpropagation and optimization)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 손실 기록 (Record loss)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 학습 과정 출력 (Print training progress)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # 테스트 데이터에 대한 성능 평가 (Evaluate on test data)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test).squeeze()\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            \n",
    "            # 예측값 계산 (Calculate predictions)\n",
    "            train_preds = (outputs > 0.5).float()\n",
    "            test_preds = (test_outputs > 0.5).float()\n",
    "            \n",
    "            # 정확도 계산 (Calculate accuracy)\n",
    "            train_acc = (train_preds == y_train).float().mean()\n",
    "            test_acc = (test_preds == y_test).float().mean()\n",
    "        \n",
    "        print(f'에포크 (Epoch) [{epoch+1}/{num_epochs}], '\n",
    "              f'손실 (Loss): {loss.item():.4f}, '\n",
    "              f'학습 정확도 (Train Acc): {train_acc:.4f}, '\n",
    "              f'테스트 정확도 (Test Acc): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가 및 임베딩 시각화 (Evaluate Model and Visualize Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 곡선 그리기 (Plot loss curve)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('학습 중 손실 변화 (Loss During Training)')\n",
    "plt.xlabel('에포크 (Epoch)')\n",
    "plt.ylabel('손실 (Loss)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 가중치 추출 (Extract embedding weights)\n",
    "embedding_weights = model.embedding.weight.cpu().detach().numpy()\n",
    "print(f\"임베딩 가중치 모양 (Embedding weights shape): {embedding_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 시각화를 위한 차원 축소 (Dimensionality reduction for embedding visualization)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA를 사용한 차원 축소 (Dimensionality reduction using PCA)\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩과 UNK 토큰 제외 (Exclude padding and UNK tokens)\n",
    "valid_embeddings = embeddings_2d[1:-1]  # 패딩과 UNK 제외 (Exclude padding and UNK)\n",
    "valid_words = list(word_to_idx.keys())[1:-1]  # 패딩과 UNK 제외 (Exclude padding and UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('단어 임베딩 PCA 시각화 (Word Embeddings PCA Visualization)')\n",
    "plt.xlabel('주성분 1 (Principal Component 1)')\n",
    "plt.ylabel('주성분 2 (Principal Component 2)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 텍스트 분류 (Classify New Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 텍스트 분류 함수 (Function to classify new text)\n",
    "def classify_text(text, model, word_to_idx, max_len=10):\n",
    "    \"\"\"\n",
    "    새로운 텍스트 분류 (Classify new text)\n",
    "    \"\"\"\n",
    "    # 토큰화 (Tokenize)\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # 인덱스 변환 (Convert to indices)\n",
    "    indices = text_to_indices(tokens, word_to_idx, max_len)\n",
    "    \n",
    "    # 텐서 변환 (Convert to tensor)\n",
    "    tensor = torch.LongTensor([indices]).to(device)\n",
    "    \n",
    "    # 예측 (Predict)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor).item()\n",
    "    \n",
    "    # 결과 해석 (Interpret result)\n",
    "    probability = output\n",
    "    prediction = 1 if probability > 0.5 else 0\n",
    "    sentiment = \"긍정(Positive)\" if prediction == 1 else \"부정(Negative)\"\n",
    "    \n",
    "    return sentiment, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 텍스트 테스트 (Test new text)\n",
    "new_texts = [\n",
    "    \"이 상품은 매우 좋아요\",  \n",
    "    \"서비스가 형편없었습니다\", \n",
    "    \"생각보다 나쁘지 않았어요\",\n",
    "    \"전반적으로 만족스럽습니다\"\n",
    "]\n",
    "\n",
    "for text in new_texts:\n",
    "    sentiment, probability = classify_text(text, model, word_to_idx)\n",
    "    print(f\"텍스트 (Text): {text}\")\n",
    "    print(f\"감정 (Sentiment): {sentiment}\")\n",
    "    print(f\"확률 (Probability): {probability:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
